---
layout: post
title: "엘라스틱서치로 인기검색어 모니터링 해보자 (2) - 엘라스틱서치에 데이터 예쁘게 쌓기"
date: 2020-03-24 02:59:12
categories: Elasticsearch
author: bactoria

---

지난번에 6주간 했던 푸드트럭 프로젝트는 Nginx를 이용하면서 액세스 로그를 쌓아왔기에, 이를 Kibana로 시각화하고 싶었다.

목표는  이용자들의 인기검색어를 Kibana로 모니터링 해보는 것.

이전글 [(엘라스틱서치로 인기검색어 모니터링 해보자 (1) - 엘라스틱서치에 액세스로그 쌓기)]([https://bactoria.github.io/2020/03/24/%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1%EC%84%9C%EC%B9%98%EB%A1%9C-%EC%9D%B8%EA%B8%B0%EA%B2%80%EC%83%89%EC%96%B4-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%ED%95%B4%EB%B3%B4%EC%9E%90-(1)-%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1%EC%84%9C%EC%B9%98%EC%97%90-%EC%95%A1%EC%84%B8%EC%8A%A4%EB%A1%9C%EA%B7%B8-%EC%8C%93%EA%B8%B0/](https://bactoria.github.io/2020/03/24/엘라스틱서치로-인기검색어-모니터링-해보자-(1)-엘라스틱서치에-액세스로그-쌓기) 에서 액세스로그를 Elasticsearch로 저장하였고, Kibana로 저장된 데이터를 확인하였다.

---

이번 포스팅에서 해결할 것은 2가지다.

- 1. 검색어가 무엇인지를 알아야 한다.
- 2. 수많은 Document 중에서 검색을 수행한 Document만 가져와야 한다.



이 두가지 문제를 해결하기 위한 해법은 message 필드에 담겨있다.

![image](https://user-images.githubusercontent.com/25674959/77336457-e2f6af80-6d6a-11ea-99f5-f6270dca801b.png)

1. **/api/trucks/search**로 시작하는 것은 **검색을 수행한 Document** 이다. (초록)

2. **/api/trucks/search?q=** 이후에 오는 것은 **검색어** 이다. (보라)

&nbsp;

인기검색어를 보기 위해 필요한 정보들이 message 필드에 있다는 것은 이제 알겠다.   

그런데 이대로 검색 쿼리를 만들자니 매우 복잡해 보인다.  

현재 인덱스 상태는 rdb에서 하나의 컬럼에 여러 정보를 넣은 것과 다름 없다.



**message 안에 짱박혀있는 유의미한 정보들을 각각 의미에 맞게 필드들로 추출하자!!**  

어떻게? => **Logstash**



Logstash 설정파일 구성은 다음과 같다.

```nginx
input {...} # 데이터를 가져온다.
filter {} # 데이터를 가공한다.
output {...} # 데이터를 내보낸다.
```

즉, filter 에서 해결할 수 있다.



---



## Version 1. message 필드 쪼개기

### 패턴 만들기

**/usr/share/logstash/patterns/nginx.pattern**

```nginx
NGINX_ACCESS %{IPORHOST:remote_addr} - %{USERNAME:remote_user} \[%{HTTPDATE:time_local}\] \"%{DATA:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{INT:status} %{NUMBER:bytes_sent} \"%{DATA:http_referer}\" \"%{DATA:http_user_agent}\"
```

Nginx의 Access.log 의 패턴을 지정한다.

- ${IPORHOST:remote_addr} : 타입: IP/Host, 이름: remote_addr

&nbsp;

**/etc/logstash/conf.d/logstash-test.conf**

```json
input {...}

filter { 
	grok { 
		patterns_dir => "/usr/share/logstash/patterns" # 1)
		match => { "message" => "%{NGINX_ACCESS}" } # 2)
		add_field => [ "received_at", "%{@timestamp}" ] # 3)
	}
}

output {...}
```

- 1) 패턴을 가져올 디렉토리
- 2) message 필드를 NGINX_ACCESS 패턴을 사용하여 쪼개겠다.
- 3) 'received_at' 필드 추가 (value: @timestamp)

&nbsp;

#### ( Logstash 재실행 )

```bash
> sudo service logstash restart
> curl -XGET 'https://<<도메인>>/api/trucks/search?q=%EA%BC%AC%EC%A7%80' # 트래픽 1 발생
```



![image](https://user-images.githubusercontent.com/25674959/77340816-2d7b2a80-6d71-11ea-8b17-2d0d6cc08915.png)

Logstash를 재실행 한 이후의 로그들은 위와 같이 나온다.

~~(DB가 죽어있어서 status가 499로 뜨는건 비밀)~~



---



## Version 2. request 필드 쪼개기

message에서 필드들을 잘 쪼갠 것 같다. 

아직 쪼개야 하는 필드가 보인다.

request 필드에는 URIPATH와 URIPARAM이 함께 들어가 있다.

&nbsp;

**/etc/logstash/conf.d/logstash-test.conf**

```json
input {...}

filter { 
	grok { 
		patterns_dir => "/usr/share/logstash/patterns"
		match => { "message" => "%{NGINX_ACCESS}" }
		add_field => [ "received_at", "%{@timestamp}" ]
	}

  # 새로 추가된 내용 1  
	grok {
    	match => { "request" => ["%{URIPATH:uri_path}"] }
	}

  # 새로 추가된 내용 2
	grok {
    	match => { "request" => ["%{URIPARAM:uri_param}"] }
	}

}

output {...}
```

기본적으로 사용 가능한 패턴에 대해서는 [여기](https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns#L36)에서 볼 수 있는데, URIPATH 와 URIPARAM 으로 쪼갤 수 있는 것 같아 보인다.

&nbsp;

#### ( Logstash 재실행 )

```bash
> sudo service logstash restart
> curl -XGET 'https://<<도메인>>/api/trucks/search?q=%EA%BC%AC%EC%A7%80' # 트래픽 1 발생
```

&nbsp;

![image](https://user-images.githubusercontent.com/25674959/77342202-310fb100-6d73-11ea-863b-77c01f9f47c0.png)

uri_path와 uri_param으로 쪼개졌다.



---



## Version 3. uri_param 필드 쪼개기

원하는 대로 잘 나눠졌지만.. url_param에 아쉬움이 남는다.

1. **?**, **=**는 불필요하며,
2. key와 value를 나눌 수 있을 것 같고,
3. 복수개의 params가 들어올 경우 이것도 나눠줘야 할 것 같다.

&nbsp;

**/etc/logstash/conf.d/logstash-test.conf**

```json
input {...}

filter { 
	grok { 
		patterns_dir => "/usr/share/logstash/patterns"
		match => { "message" => "%{NGINX_ACCESS}" }
		add_field => [ "received_at", "%{@timestamp}" ]
	}
	grok {
    	match => { "request" => ["%{URIPATH:uri_path}"] }
	}
	grok {
    	match => { "request" => ["%{URIPARAM:uri_param}"] }
	}

	# 새로 추가된 내용
	kv {
    source => "request"
    field_split => "&?"
    value_split => "="
	  target => "params"
	}
}

output {...}
```

1. request 필드에서
   - ?q=%EA%BC%AC%EC%A7%80&limit=10
2. &과 ? 로 쪼개고 
   - [ q=%EA%BC%AC%EC%A7%80 , limit=10 ]
3. 각각을 = 기준으로 쪼개서 왼쪽은 key, 오른쪽은 value
   - {q: %EA%BC%AC%EC%A7%80}, {limit: 10}
4. params 라는 필드에 저장해달라
   - params = {q: %EA%BC%AC%EC%A7%80, limit: 10}

&nbsp;

#### ( Logstash 재실행 )

```bash
> sudo service logstash restart
> curl -XGET 'https://<<도메인>>/api/trucks/search?q=%EA%BC%AC%EC%A7%80&limit=10' # 트래픽 1 발생
```

&nbsp;

![image](https://user-images.githubusercontent.com/25674959/77344009-e5123b80-6d75-11ea-9f80-1ff3281b94f2.png)



위 그림에서 필드의 타입이 ? 인 것들이 많다.

Refresh를 누르면 Elasticsearch로 부터 필드 타입 정보들을 받아올 수 있다.

**Management - Index Patterns - nginx* - Refresh field list**



![image](https://user-images.githubusercontent.com/25674959/77379283-aa32f680-6dbb-11ea-885c-b0c1ad7c19ff.png)

**?** 가 사라졌다.

### 검색어 가져오기 끗



---



## 검색한 Document만 가져오기

=> **uri_path** 가 **/api/trucks/search** 인 것만 가져오면 된다.



![image](https://user-images.githubusercontent.com/25674959/77347097-acc12c00-6d7a-11ea-8e0b-34d4ec40126e.png)

### 검색한 Document 가져오기 끗



---



TODO :: 필드 타입 설정 키바나에 반영, 인기검색어 Visualize, 형태소 분석